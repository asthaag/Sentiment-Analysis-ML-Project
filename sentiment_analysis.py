# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xgWh6AMjmLtUF74JtGK7007uMVJNwfVH
"""

# installing kaggle library
! pip install kaggle

# configure the path of kaggle.json file

  !mkdir -p ~/.kaggle
  !cp kaggle.json ~/.kaggle/
  !chmod 600 ~/.kaggle/kaggle.json

# API to fetch the dataset from kaggle

!kaggle datasets download -d kazanova/sentiment140
# uploading this file would have taken longer time

# extracting the compressed dataset

from zipfile import ZipFile
dataset = '/content/sentiment140.zip'

with ZipFile(dataset, 'r') as zip:     # 'r' means reading as zip
  zip.extractall()
  print('The dataset is extracted')

"""Importing the Dependencies"""

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression  # as this is classification between positive and negative tweets
from sklearn.metrics import accuracy_score     # will test the accuracy of our ML model

import nltk
nltk.download('stopwords')

# printing the stopwords in English
print(stopwords.words('english'))
# these are the words which does not add much meaning to the texts, so we will remove these from our data

"""Data Processing"""

# loading the DATA from csv file to pandas dataframe
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding = 'ISO-8859-1')

# checking the number of rows and columns

twitter_data.shape

# printing the first 5 rows of the dataframe

twitter_data.head()

# here column names have not been read , so the first row has become the column names here

# naming the columns and reading the dataset again

column_names = ['target','id','date','flag','user','text']

twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', names = column_names, encoding = 'ISO-8859-1')

twitter_data.shape

twitter_data.head()

# counting the number of missing values in the dataset

twitter_data.isnull().sum()

# no values are missing

# checking the distribution of target column

twitter_data['target'].value_counts()
# if we do not have equal distribution of the values, we do uplabelling and downlabelling, otherwise our ML model won't work correctly

"""Convert the target "4" to "1"
"""

twitter_data.replace({'target':{4:1}}, inplace = True)

twitter_data['target'].value_counts()

"""0 --> Negative Tweet
1 --> Positive Tweet

**Stemming**

Stemming is the process of reducing a word to its Root word

example : actor, actress, acting = act
"""

port_stem = PorterStemmer()

def stemming(content):

  stemmed_content = re.sub('[^a-zA-Z]',' ',content)     # ^ means remove everything from text which is not alphabet
  stemmed_content = stemmed_content.lower()      # converting all letters to lowercase letters
  stemmed_content = stemmed_content.split()      # split all the words in the tweet and store them in a list
  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  # reduce the words to root word
  stemmed_content = ' '.join(stemmed_content)    # join the words which were there in the list to form a single tweet

  return stemmed_content

# creating new column
twitter_data['stemmed_content'] = twitter_data['text'].apply(stemming)
# 50 minutes to complete this execution

twitter_data.head()

print(twitter_data['stemmed_content'])

print(twitter_data['target'])

# seperating the data and label
X = twitter_data['stemmed_content'].values
Y = twitter_data['target'].values

print(X)

print(Y)

"""Splitting the data into training and testing data"""

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, stratify = Y, random_state = 2)
# startify=Y means that i want equal no. of 0 and 1 in my training data and also in my test data

print(X.shape, X_train.shape, X_test.shape)

print(X_train)

print(X_test)

# converting the textual data to numerical data

vectorizer = TfidfVectorizer()

X_train = vectorizer.fit_transform(X_train)

X_test = vectorizer.transform(X_test)

# all words in a tweet get some importance value

print(X_train)

# (0, ) means 1st tweet words

print(X_test)

"""Training the Machine Learning Model

Logistic Regression
"""

model = LogisticRegression(max_iter=1000)

model.fit(X_train, Y_train)
# here model will learn from the training data

"""Model Evaluation

Accuracy Score
"""

# accuracy score on the training data

X_train_prediction = model.predict(X_train)     # here the model is predicting 0 or 1
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)     # here it is comparing prediction with Y_train

print('Accuracy Score on the training data : ', training_data_accuracy)

# accuracy score on the test data

X_test_prediction = model.predict(X_test)     # here the model is predicting 0 or 1
test_data_accuracy = accuracy_score(Y_test, X_test_prediction)     # here it is comparing prediction with Y_test

print('Accuracy Score on the test data : ', test_data_accuracy)

"""Model Accuracy = 77.8 %

Saving the trianed model
"""

import pickle

filename = 'trained_model.sav'
pickle.dump(model, open(filename, 'wb'))     # wb means we are writing in binary format, if r then we are reading

"""Using the saved model for future prediction"""

# loading the saved model

loaded_model = pickle.load(open('/content/trained_model.sav', 'rb'))

X_new = X_test[200]
print(Y_test[200])

prediction = loaded_model.predict(X_new)
print(prediction)

if (prediction[0] == 0):
  print('Negative Tweet')

else:
  print('Positive Tweet')

X_new = X_test[3]
print(Y_test[3])

prediction = loaded_model.predict(X_new)
print(prediction)

if (prediction[0] == 0):
  print('Negative Tweet')

else:
  print('Positive Tweet')

